---
title: "Recognition of quality of weight lifting exercises using a random forest approach"
output: html_document
---



Summary
-------

The task was to use inertial measurement unit data to learn whether a bicep curl was being performed correctly (label A) or in one of four other incorrect ways (labelled B,C,D or E).  

The data and its collection was initially described here http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf. 

Data was prepared for learning using only very simple cleaning methods to ensure that each feature was available for every sample (i.e. no NAs in data).  A random forest was trained.  Out of sample error was estimated to be 0.53% as assessed by accuracy performance on validation data held out of the initial training data.  20 out of 20 test samples were correctly classified.


Preparing the work environment
------------------------------

Libraries required for learning were installed and loaded.

```{r load.libs, results='hide', message=FALSE}

# install.packages("rpart")
# install.packages("rpart.plot")
# install.packages("rattle")
# install.packages("randomForest")
# install.packages("caret")

library(rpart)
library(rattle)
library(randomForest)
library(caret)
```


Data retrieval
--------------

Training data was obtained from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv. 
Testing data was obtained from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv.  For the purpose of this report, the data is included as part of the git repository to ensure reproducibility.

```{r data, cache=TRUE}

training <- read.table(file="pml-training.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
testing <-  read.table(file="pml-testing.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)

```


Preprocessing data
------------------

A single function was written to preprocess training and testing data identically.  Briefly, all data columns were cast as numerics.  Meta-data columns containing data irrelevant to learning the class were removed (columns 1 to 7). Columns containing any NAs were removed.  The class column was cast as a factor.  

Checks were performed to ensure that training and testing data had no NAs and that they had identical data structures.

```{r clean, results='hide', warning=FALSE, cache=TRUE}

clean <- function(theseData) {
  #cast all the "character" data columns to numerics
  for (i in 1:length(theseData)){
    if (i > 7 && i < 160 && is.character(theseData[1,i])){
      cat(paste("converting ", colnames(theseData[i]), " to numeric"))
      theseData[,i] = as.numeric(theseData[,i])
      count <- sum(theseData[,i], na.rm=TRUE)
      cat(paste(" which now has a column sum of ", count, "\n"))
      }
    else {
      cat(paste(i, " is ok\n"))
      }
    }
  
#remove columns 1:7 and any with NAs
numNAs <- apply(theseData, 2, function(x) sum(is.na(x)))
noNAs <- which(numNAs == 0)
theseColumns <- noNAs[8:length(noNAs)]
theseData2 <- theseData[,theseColumns]

#change the classe variable to a factor (last column in both training and testing data sets)
lastColumn <- length(theseData2)
theseData2[,lastColumn] <- as.factor(theseData2[,lastColumn])

return(theseData2)
}


#clean training data
cleanTraining <- clean(training)
dim(cleanTraining)
sum(is.na(cleanTraining))
str(cleanTraining)

#clean testing data
cleanTesting <- clean(testing)
dim(testing)
sum(is.na(cleanTesting))
str(cleanTesting)


```

Data partitioning
-----------------

Training data was partitioned into training data (70%) and validation data (30%) subsets.


```{r partition}

thisTraining <- cleanTraining

## Create training and test sets
set.seed(123)
inTrain <- createDataPartition(y=thisTraining$classe, p=0.7, list=FALSE)
training <- thisTraining[inTrain,]
validation <- thisTraining[-inTrain,]
dim(training)
dim(validation)

```

Learning
--------

Multiple methods and packages were assessed for learning.  A simple decision tree was attempted first.

```{r decisionTree, results='hide', cache=TRUE}

# train the model
modFit <- train(classe ~ ., method="rpart", 
                trControl=trainControl(method='cv', number=5, verboseIter=TRUE, allowParallel=TRUE), 
                data=training) #<---rpart is an r package for decision tree learning

## plot decision tree
fancyRpartPlot(modFit$finalModel)

## predicting new values
predictions <- predict(modFit,newdata=validation)  
```
```{r decisionTree.results, cache=TRUE}
# make the confusion matrix
confusionMatrix(predictions,validation$classe)
```

Performance was poor - with an overall accuracy of only 0.552 even though sensitivity was above random for classes A and C (0.634 and 0.798 respectively.

A random forest approach was used second.  The mtry parameter of the the randomForest method was first optimized using tuneRF.  

```{r randomForest, cache=TRUE}

#try directly using randomForest - see tutorial at http://scg.sdsu.edu/rf_r/
#first assess best value for mtry
best.mtry <- tuneRF(training[-53], 
                    training$classe, 
                    ntreeTry=100, 
                    stepFactor=1.5, 
                    improve=0.01, 
                    trace=TRUE,
                    plot=TRUE,
                    dobest=FALSE)

modfit <- randomForest(classe ~ ., data=training, mtry=10, ntree=1000, keep.forest=TRUE, importance=TRUE, test=testing)
#runtime less than 10 minutes - MacBook Air, 1.8 Ghz Intel i5, 4GB RAM

modfit

```
The out-of-bag error rate was estimated to be 0.53%.

Out of sample error rate was also estimated using an unseen validation set.
```{r validation, cache=TRUE}

#make preditions on validation data
predictions <- predict(modfit,validation)

#make the confusion matrix
confusionMatrix(predictions,validation$classe)



```

In this case, the out of sample error was estimated to be 0.5%.

The model was used to predict classifications for the 20 samples in the test set.  These predictions were prepared for submission.  All 20 submissions were correct.

```{r submit.answers}

#apply the model to the real test data
answers <- predict(modfit,testing)

#########submit the answers - see https://class.coursera.org/predmachlearn-004/assignment/view?assignment_id=5

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)

```